import os
import json
import time
import torch
import subprocess
import sys
import shutil
import google.generativeai as genai
from fastapi import FastAPI, HTTPException, BackgroundTasks, UploadFile, File
from pydantic import BaseModel
from fastapi.responses import StreamingResponse
from typing import List, Optional, Dict

# --- 1. CONFIGURATION ---
API_KEY = "AIzaSyDdazQHNvH33BRfB6b_rG898R5uAvukQ60"
genai.configure(api_key=API_KEY)
GEMINI_MODEL_NAME = "gemini-2.0-flash-thinking-exp-01-21"
model = genai.GenerativeModel(GEMINI_MODEL_NAME)

# PATHS
FLASH_RAG_PATH = r'J:\codigo\FlashRAG-main'
DATA_FOLDER = "data"
CORPUS_FILENAME = "uploaded_docs.jsonl"
CORPUS_PATH = os.path.join(DATA_FOLDER, CORPUS_FILENAME)
INDEX_SAVE_DIR = "../flashrag_index/"
INDEX_PATH = os.path.join(INDEX_SAVE_DIR, "bge_Flat.index")

app = FastAPI(title="Gemini Agent Factory")

# --- 2. LOAD RAG SYSTEM ---
# (Same RAG loading logic as before - kept brief for clarity)
print("Loading RAG System...")
if FLASH_RAG_PATH not in sys.path:
    sys.path.append(FLASH_RAG_PATH)
retriever = None


def load_retriever():
    global retriever
    try:
        from flashrag.config import Config
        from flashrag.utils import get_retriever
        if os.path.exists(INDEX_PATH):
            config_dict = {
                "retrieval_method": "bge",
                "retrieval_topk": 5,
                "corpus_path": CORPUS_PATH,
                "index_path": INDEX_PATH,
                "instruction": "Represent this sentence for searching relevant passages:",
                "device": "cuda" if torch.cuda.is_available() else "cpu"
            }
            config = Config(config_dict=config_dict)
            retriever = get_retriever(config)
            print("✅ RAG System Loaded Successfully")
        else:
            print("⚠️ Index file not found. RAG is disabled.")
            retriever = None
    except Exception as e:
        print(f"❌ Error loading FlashRAG: {e}")
        retriever = None


load_retriever()


# --- 3. DATA MODELS ---

# This matches the data structure you will send from your TypeScript backend
class AgentConfig(BaseModel):
    name: str
    mode: str  # "PERFORMANCE" or "EFFICIENCY"
    instructions: List[str]  # List of raw instruction strings from the DB
    tools: List[str]  # List of tool names (e.g., ["RAG", "Calculator"])


class ChatRequest(BaseModel):
    message: str
    system_prompt: str  # The compiled prompt generated by the optimizer


# --- 4. THE REFERENCE LOGIC (V5 Template) ---
ANALYSIS_V5_TEMPLATE = (
    "--- ROUTING LOGIC (V5) ---\n"
    "You are a query classification expert. Classify the query into Profile 1 or Profile 2.\n"
    "--- PROFILE 1: RETRIEVAL REQUIRED ('1-Yes') ---\n"
    "Choose this if the query requires external, specific, or up-to-date knowledge.\n"
    "--- PROFILE 2: DIRECT ANSWER SUFFICIENT ('2-No') ---\n"
    "Choose this if the query is self-contained, common knowledge, or creative.\n"
    "**Decision Rule:** If in doubt, choose '1-Yes'.\n"
    "**IMPORTANT** Reply ONLY with '1-Yes' or '2-No'."
)


# --- 5. THE AGENT COMPILER (Prompt Optimizer) ---

@app.post("/optimize_prompt")
async def optimize_prompt(config: AgentConfig):
    """
    Compiles the Agent's Name, Mode, Tools, and Instructions into a single System Prompt.
    """

    # 1. Determine Mode Behavior
    mode_instruction = ""
    if config.mode == "PERFORMANCE":
        mode_instruction = "You are in PERFORMANCE mode. Prioritize accuracy, detail, and thoroughness over speed. Always double-check your reasoning."
    elif config.mode == "EFFICIENCY":
        mode_instruction = "You are in EFFICIENCY mode. Prioritize concise, direct answers. Avoid unnecessary elaboration or retrieval if not strictly needed."

    # 2. Format Tools
    tools_list = ", ".join(config.tools) if config.tools else "No external tools available."

    # 3. Format Raw Instructions
    raw_instructions = "\n".join(f"- {instr}" for instr in config.instructions)

    # 4. Construct the Meta-Prompt for Gemini
    meta_prompt = (
        "You are an expert AI Architect. Your task is to compile a final SYSTEM PROMPT for a new AI Agent based on the configuration below.\n\n"

        "--- AGENT CONFIGURATION ---\n"
        f"Name: {config.name}\n"
        f"Mode: {config.mode} ({mode_instruction})\n"
        f"Available Tools: {tools_list}\n"
        f"User-Provided Guidelines:\n{raw_instructions}\n"
        "---------------------------\n\n"

        "--- REFERENCE ROUTING LOGIC ---\n"
        f"{ANALYSIS_V5_TEMPLATE}\n"
        "-------------------------------\n\n"

        "**TASK:**\n"
        "Generate a single, cohesive SYSTEM PROMPT that the agent will use to operate. The prompt must include:\n"
        "1. **Persona Definition:** Incorporate the Name and Mode.\n"
        "2. **Operational Guidelines:** Integrate the User-Provided Guidelines into clear rules.\n"
        "3. **Tool Usage & Routing:** Adapt the 'Reference Routing Logic' to this specific agent. \n"
        "   - If the agent has the 'RAG' tool, define specific examples for Profile 1 (Retrieval) based on the agent's domain.\n"
        "   - If the agent does NOT have 'RAG', remove the routing logic and instruct it to rely on internal knowledge.\n\n"

        "Output ONLY the final System Prompt text."
    )

    try:
        response = model.generate_content(meta_prompt)
        return {"optimized_prompt": response.text}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Gemini Error: {str(e)}")


# --- 6. RUNTIME (Chat) ---

async def decide_path(query, system_prompt):
    """
    Uses the compiled system prompt to decide the path.
    """
    routing_check = f"""
    {system_prompt}

    [TASK]
    Analyze the following query based on the Routing Logic defined above.
    Does this query fit Profile 1 (Retrieval Required) or Profile 2 (Direct Answer)?

    Query: {query}

    Reply ONLY with '1-Yes' or '2-No'.
    """
    try:
        response = model.generate_content(routing_check)
        decision = response.text.strip()
        if "1-Yes" in decision or "1-yes" in decision:
            return "retrieval"
        return "direct"
    except:
        return "direct"


@app.post("/generate_stream")
async def generate_stream(request: ChatRequest):
    # 1. Decide Path
    method = await decide_path(request.message, request.system_prompt)

    # 2. Execute Path
    context_str = ""
    retrieved_docs = []

    if method == "retrieval":
        # Only retrieve if RAG is actually available/loaded
        if retriever:
            try:
                results = retriever.search(request.message)
                retrieved_docs = [doc['contents'] for doc in results]
                context_str = "\n".join(retrieved_docs)
            except:
                context_str = "Error retrieving documents."
        else:
            context_str = "RAG tool is not active."

    # 3. Build Final Prompt
    full_prompt = f"""
    [SYSTEM PROMPT]
    {request.system_prompt}

    [EXECUTION CONTEXT]
    Routing Decision: {method.upper()}

    [RETRIEVED KNOWLEDGE]
    {context_str if context_str else "No external context used."}

    [USER MESSAGE]
    {request.message}
    """

    # 4. Stream Response
    async def response_generator():
        start_time = time.time()
        total_tokens = 0

        yield json.dumps({"text": "", "is_final": False, "metrics": {"method_used": method}}) + "\n"

        try:
            stream = model.generate_content(full_prompt, stream=True)
            for chunk in stream:
                if chunk.text:
                    total_tokens += len(chunk.text) / 4
                    yield json.dumps({"text": chunk.text, "is_final": False,
                                      "metrics": {"current_tokens": int(total_tokens)}}) + "\n"

            duration = time.time() - start_time
            yield json.dumps({
                "text": "",
                "is_final": True,
                "metrics": {
                    "total_tokens": int(total_tokens),
                    "duration_sec": round(duration, 2),
                    "retrieved_docs": retrieved_docs
                }
            }) + "\n"

        except Exception as e:
            yield json.dumps({"error": str(e)}) + "\n"

    return StreamingResponse(response_generator(), media_type="application/x-ndjson")


# --- 7. UPLOAD (Same as before) ---
@app.post("/upload_and_index")
async def upload_and_index(background_tasks: BackgroundTasks, file: UploadFile = File(...)):
    try:
        os.makedirs(DATA_FOLDER, exist_ok=True)
        with open(CORPUS_PATH, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)

        def run_indexing():
            subprocess.run([
                "python", "-m", "flashrag.retriever.index_builder",
                "--retrieval_method", "bge",
                "--model_path", "BAAI/bge-base-en-v1.5",
                "--corpus_path", CORPUS_PATH,
                "--save_dir", INDEX_SAVE_DIR,
                "--faiss_type", "Flat",
                "--instruction", ""
            ], cwd=FLASH_RAG_PATH)
            load_retriever()

        background_tasks.add_task(run_indexing)
        return {"status": "accepted", "message": "Indexing started."}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))